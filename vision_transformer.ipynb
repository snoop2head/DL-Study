{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "- [Vision Transformers Youtube Video by mildlyoverfitted](https://www.youtube.com/watch?v=ovB0ddFtzzA&t=206s)\n",
    "  - [Vision transformers implementation by mildlyoverfitted](https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/vision_transformer)\n",
    "  - [Code on the video](https://github.com/MignonDeveloper/pytorch-tutorials/blob/master/2.%20Computer%20Vision/Classification/Vision%20Transformer/model.py)\n",
    "- [Pooling-based vision transformers implementation in Pytorch](https://github.com/rwightman/pytorch-image-models/blob/79927baaecb6cdd1a25eed7f0f8c122b99712c72/timm/models/pit.py)\n",
    "- [lucidrains/vit-pytorch implementation](https://github.com/lucidrains/vit-pytorch)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "  \"\"\" Split image into patches and then embed them \n",
    "  \n",
    "  Arguments:\n",
    "    img_size (int): Size of the image (it is a square).\n",
    "    patch_size (int): Size of the patch (it is a square).\n",
    "    input_channels (int): number of input channels(RGB: 3, Grayscale: 1)\n",
    "    embed_dim (int): the embedding dimension\n",
    "  \n",
    "  Attributes:\n",
    "    num_patches (int): number of patches inside of our image\n",
    "    proj (nn.Conv2d): Convolutional layer that does both splitting into patches and thier embedding\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, img_size, patch_size, input_channels=3, embed_dim=768):\n",
    "    super(PatchEmbed, self).__init__()\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.input_channels = input_channels\n",
    "    self.embed_dim = embed_dim\n",
    "\n",
    "    self.n_patches = (img_size // patch_size) ** 2\n",
    "    self.proj = nn.Conv2d(input_channels, self.n_patches * embed_dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('aitech': pyenv)"
  },
  "interpreter": {
   "hash": "0964e3f95126ddd6db105c73e10d4f1245c8e331a9ca081f1675c5dba7db3ce0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}