{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Gradient Descent using sympy library\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# SymPy is a Python library for symbolic mathematics\n",
    "# https://www.sympy.org/en/index.html\n",
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "\n",
    "def make_function_and_value(input_algebraic_notation, input_value):\n",
    "  \"\"\" yields both polynomial function and plugin value \"\"\"\n",
    "  \n",
    "  # define polynomial function\n",
    "  polynomial_function = sym.poly(input_algebraic_notation) # x is predefined variable in sympy.abc\n",
    "  \n",
    "  # get value by plugging in input_value to x\n",
    "  plugged_in_value = polynomial_function.subs(x, input_value)\n",
    "  \n",
    "  # return both the function and the output value\n",
    "  return plugged_in_value, polynomial_function\n",
    "\n",
    "def yield_gradient_function_and_value(input_function, input_value):\n",
    "  \"\"\" yields first-order derivative function(or, gradient function) \"\"\"\n",
    "  \n",
    "  # calculate gradient of input function(or first-order derivative function) using sympy's differential method\n",
    "  gradient_function = sym.diff(input_function, x)\n",
    "\n",
    "  # get the output value by plugin given input_value to x\n",
    "  gradient_value = gradient_function.subs(x, input_value)\n",
    "\n",
    "  # return both the function and the output value\n",
    "  return gradient_value, gradient_function\n",
    "\n",
    "def gradient_descent(input_function, init_point, lr_rate=1e-2, epsilon=1e-5):\n",
    "  \"\"\" \n",
    "  gradient descent algorithm \n",
    "  - init_point: initial point\n",
    "  - lr_rate: learning rate\n",
    "  - epsilon: convergence criteria(think of epsilon and neighborhood)\n",
    "  \"\"\"\n",
    "\n",
    "  # initialize variables' default value\n",
    "  iter_cnt = 0 # count of iteration\n",
    "  current_x_val = init_point # current x value\n",
    "  gradient_value, gradient_function = yield_gradient_function_and_value(input_function, current_x_val)\n",
    "  print(\"Learning rate: {}\".format(lr_rate))\n",
    "  print(\"Initial point: {}\".format(current_x_val))\n",
    "  print(\"Gradient function: {}\".format(gradient_function))\n",
    "  print(\"Convergence criteria: {}\".format(epsilon))\n",
    "  \n",
    "  # repeat until convergence\n",
    "  while np.abs(gradient_value) > epsilon:\n",
    "    # update current point\n",
    "    current_x_val -= lr_rate * gradient_value\n",
    "\n",
    "    # update gradient of function at current(or moved) point\n",
    "    gradient_value, _ = yield_gradient_function_and_value(input_function, current_x_val)\n",
    "    \n",
    "    # show the current point\n",
    "    current_y_val = input_function.subs(x, current_x_val)\n",
    "\n",
    "    # print information on every 10 iterations\n",
    "    if iter_cnt % 10 == 0:\n",
    "      # print current point    \n",
    "      print(f\"Current point: ({current_x_val}, {current_y_val})\")\n",
    "      \n",
    "      # print iteration and gradient value(or weight delta)\n",
    "      print(\"Iteration: {}\".format(iter_cnt))\n",
    "      print(\"Gradient value: {}\".format(gradient_value))\n",
    "\n",
    "    # update count of iteration\n",
    "    iter_cnt += 1\n",
    "  \n",
    "  # return current point\n",
    "  return [current_x_val, current_y_val]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.01\n",
      "Initial point: 3\n",
      "Gradient function: Poly(2*x + 2, x, domain='ZZ')\n",
      "Convergence criteria: 1e-05\n",
      "Current point: (2.92000000000000, 17.3664000000000)\n",
      "Iteration: 0\n",
      "Gradient value: 7.84000000000000\n",
      "Current point: (2.20292540299918, 12.2587311371775)\n",
      "Iteration: 10\n",
      "Gradient value: 6.40585080599837\n",
      "Current point: (1.61702324927997, 8.84881068727189)\n",
      "Iteration: 20\n",
      "Gradient value: 5.23404649855994\n",
      "Current point: (1.13829853197915, 6.57232061186421)\n",
      "Iteration: 30\n",
      "Gradient value: 4.27659706395831\n",
      "Current point: (0.747145583487728, 5.05251768990067)\n",
      "Iteration: 40\n",
      "Gradient value: 3.49429116697546\n",
      "Current point: (0.427545145941499, 4.03788514370114)\n",
      "Iteration: 50\n",
      "Gradient value: 2.85509029188300\n",
      "Current point: (0.166408319353113, 3.36050836745615)\n",
      "Iteration: 60\n",
      "Gradient value: 2.33281663870623\n",
      "Current point: (-0.0469594805291656, 2.90828623175324)\n",
      "Iteration: 70\n",
      "Gradient value: 1.90608103894167\n",
      "Current point: (-0.221296507678400, 2.60637912895386)\n",
      "Iteration: 80\n",
      "Gradient value: 1.55740698464320\n",
      "Current point: (-0.363742551795655, 2.40482354039551)\n",
      "Iteration: 90\n",
      "Gradient value: 1.27251489640869\n",
      "Current point: (-0.480131340892568, 2.27026342272216)\n",
      "Iteration: 100\n",
      "Gradient value: 1.03973731821486\n",
      "Current point: (-0.575229455490225, 2.18043001548313)\n",
      "Iteration: 110\n",
      "Gradient value: 0.849541089019550\n",
      "Current point: (-0.652931538914246, 2.12045651668043)\n",
      "Iteration: 120\n",
      "Gradient value: 0.694136922171507\n",
      "Current point: (-0.716419798318522, 2.08041773078571)\n",
      "Iteration: 130\n",
      "Gradient value: 0.567160403362956\n",
      "Current point: (-0.768294328634378, 2.05368751814299)\n",
      "Iteration: 140\n",
      "Gradient value: 0.463411342731244\n",
      "Current point: (-0.810679596725528, 2.03584221509601)\n",
      "Iteration: 150\n",
      "Gradient value: 0.378640806548945\n",
      "Current point: (-0.845311446695445, 2.02392854852346)\n",
      "Iteration: 160\n",
      "Gradient value: 0.309377106609111\n",
      "Current point: (-0.873608189558073, 2.01597488974679)\n",
      "Iteration: 170\n",
      "Gradient value: 0.252783620883854\n",
      "Current point: (-0.896728688674616, 2.01066496374286)\n",
      "Iteration: 180\n",
      "Gradient value: 0.206542622650768\n",
      "Current point: (-0.915619819784411, 2.00712001481322)\n",
      "Iteration: 190\n",
      "Gradient value: 0.168760360431179\n",
      "Current point: (-0.931055249305571, 2.00475337864832)\n",
      "Iteration: 200\n",
      "Gradient value: 0.137889501388858\n",
      "Current point: (-0.943667119029941, 2.00317339347839)\n",
      "Iteration: 210\n",
      "Gradient value: 0.112665761940119\n",
      "Current point: (-0.953971934825732, 2.00211858278369)\n",
      "Iteration: 220\n",
      "Gradient value: 0.0920561303485365\n",
      "Current point: (-0.962391719592458, 2.00141438275521)\n",
      "Iteration: 230\n",
      "Gradient value: 0.0752165608150845\n",
      "Current point: (-0.969271296765196, 2.00094425320249)\n",
      "Iteration: 240\n",
      "Gradient value: 0.0614574064696090\n",
      "Current point: (-0.974892412195924, 2.00063039096534)\n",
      "Iteration: 250\n",
      "Gradient value: 0.0502151756081526\n",
      "Current point: (-0.979485272758748, 2.00042085403378)\n",
      "Iteration: 260\n",
      "Gradient value: 0.0410294544825043\n",
      "Current point: (-0.983237974230458, 2.00028096550790)\n",
      "Iteration: 270\n",
      "Gradient value: 0.0335240515390847\n",
      "Current point: (-0.986304204555359, 2.00018757481286)\n",
      "Iteration: 280\n",
      "Gradient value: 0.0273915908892828\n",
      "Current point: (-0.988809537973489, 2.00012522644037)\n",
      "Iteration: 290\n",
      "Gradient value: 0.0223809240530217\n",
      "Current point: (-0.990856577781630, 2.00008360216986)\n",
      "Iteration: 300\n",
      "Gradient value: 0.0182868444367394\n",
      "Current point: (-0.992529158343479, 2.00005581347506)\n",
      "Iteration: 310\n",
      "Gradient value: 0.0149416833130425\n",
      "Current point: (-0.993895778437894, 2.00003726152088)\n",
      "Iteration: 320\n",
      "Gradient value: 0.0122084431242127\n",
      "Current point: (-0.995012406554386, 2.00002487608838)\n",
      "Iteration: 330\n",
      "Gradient value: 0.00997518689122723\n",
      "Current point: (-0.995924773023779, 2.00001660747491)\n",
      "Iteration: 340\n",
      "Gradient value: 0.00815045395244285\n",
      "Current point: (-0.996670242855835, 2.00001108728264)\n",
      "Iteration: 350\n",
      "Gradient value: 0.00665951428833012\n",
      "Current point: (-0.997279345983963, 2.00000740195827)\n",
      "Iteration: 360\n",
      "Gradient value: 0.00544130803207366\n",
      "Current point: (-0.997777027586547, 2.00000494160635)\n",
      "Iteration: 370\n",
      "Gradient value: 0.00444594482690586\n",
      "Current point: (-0.998183669690506, 2.00000329905579)\n",
      "Iteration: 380\n",
      "Gradient value: 0.00363266061898715\n",
      "Current point: (-0.998515925895787, 2.00000220247595)\n",
      "Iteration: 390\n",
      "Gradient value: 0.00296814820842539\n",
      "Current point: (-0.998787403406042, 2.00000147039050)\n",
      "Iteration: 400\n",
      "Gradient value: 0.00242519318791623\n",
      "Current point: (-0.999009220297352, 2.00000098164442)\n",
      "Iteration: 410\n",
      "Gradient value: 0.00198155940529543\n",
      "Current point: (-0.999190460847351, 2.00000065535364)\n",
      "Iteration: 420\n",
      "Gradient value: 0.00161907830529895\n",
      "Current point: (-0.999338547572259, 2.00000043751931)\n",
      "Iteration: 430\n",
      "Gradient value: 0.00132290485548103\n",
      "Current point: (-0.999459545208244, 2.00000029209138)\n",
      "Iteration: 440\n",
      "Gradient value: 0.00108090958351292\n",
      "Current point: (-0.999558409086304, 2.00000019500254)\n",
      "Iteration: 450\n",
      "Gradient value: 0.000883181827392221\n",
      "Current point: (-0.999639188072650, 2.00000013018525)\n",
      "Iteration: 460\n",
      "Gradient value: 0.000721623854699427\n",
      "Current point: (-0.999705190385762, 2.00000008691271)\n",
      "Iteration: 470\n",
      "Gradient value: 0.000589619228476224\n",
      "Current point: (-0.999759119080997, 2.00000005802362)\n",
      "Iteration: 480\n",
      "Gradient value: 0.000481761838005967\n",
      "Current point: (-0.999803182751385, 2.00000003873703)\n",
      "Iteration: 490\n",
      "Gradient value: 0.000393634497230400\n",
      "Current point: (-0.999839185978230, 2.00000002586115)\n",
      "Iteration: 500\n",
      "Gradient value: 0.000321628043539857\n",
      "Current point: (-0.999868603235846, 2.00000001726511)\n",
      "Iteration: 510\n",
      "Gradient value: 0.000262793528308602\n",
      "Current point: (-0.999892639277096, 2.00000001152632)\n",
      "Iteration: 520\n",
      "Gradient value: 0.000214721445807253\n",
      "Current point: (-0.999912278472788, 2.00000000769507)\n",
      "Iteration: 530\n",
      "Gradient value: 0.000175443054424296\n",
      "Current point: (-0.999928325125536, 2.00000000513729)\n",
      "Iteration: 540\n",
      "Gradient value: 0.000143349748927157\n",
      "Current point: (-0.999941436409139, 2.00000000342969)\n",
      "Iteration: 550\n",
      "Gradient value: 0.000117127181722188\n",
      "Current point: (-0.999952149282434, 2.00000000228969)\n",
      "Iteration: 560\n",
      "Gradient value: 0.0000957014351326002\n",
      "Current point: (-0.999960902479887, 2.00000000152862)\n",
      "Iteration: 570\n",
      "Gradient value: 0.0000781950402268805\n",
      "Current point: (-0.999968054479499, 2.00000000102052)\n",
      "Iteration: 580\n",
      "Gradient value: 0.0000638910410024618\n",
      "Current point: (-0.999973898183897, 2.00000000068130)\n",
      "Iteration: 590\n",
      "Gradient value: 0.0000522036322065933\n",
      "Current point: (-0.999978672915852, 2.00000000045484)\n",
      "Iteration: 600\n",
      "Gradient value: 0.0000426541682967407\n",
      "Current point: (-0.999982574219492, 2.00000000030366)\n",
      "Iteration: 610\n",
      "Gradient value: 0.0000348515610157918\n",
      "Current point: (-0.999985761868608, 2.00000000020272)\n",
      "Iteration: 620\n",
      "Gradient value: 0.0000284762627837942\n",
      "Current point: (-0.999988366410019, 2.00000000013534)\n",
      "Iteration: 630\n",
      "Gradient value: 0.0000232671799620388\n",
      "Current point: (-0.999990494509980, 2.00000000009035)\n",
      "Iteration: 640\n",
      "Gradient value: 0.0000190109800397575\n",
      "Current point: (-0.999992233322589, 2.00000000006032)\n",
      "Iteration: 650\n",
      "Gradient value: 0.0000155333548226277\n",
      "Current point: (-0.999993654059087, 2.00000000004027)\n",
      "Iteration: 660\n",
      "Gradient value: 0.0000126918818252886\n",
      "Current point: (-0.999994814904246, 2.00000000002689)\n",
      "Iteration: 670\n",
      "Gradient value: 0.0000103701915079579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.999995020234038, 2.00000000002480]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy.abc import x\n",
    "\n",
    "# get random initial x value\n",
    "x_starting_pt = np.random.randint(1, 10)\n",
    "\n",
    "# define polynomial function\n",
    "_, polynomial_function = make_function_and_value(x**2 + 2*x + 3, x_starting_pt)\n",
    "\n",
    "gradient_descent(input_function = polynomial_function, init_point = x_starting_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Weight: -15.614135767290168\n",
      "Bias: -15.614135767290168\n",
      "Error: 4426978.411558665\n",
      "Iteration: 10\n",
      "Weight: -249329140494.23785\n",
      "Bias: -249329140494.23785\n",
      "Error: 5.60865920479758e+26\n",
      "Iteration: 20\n",
      "Weight: -3.5975295387343876e+21\n",
      "Bias: -3.5975295387343876e+21\n",
      "Error: 1.1676742667972743e+47\n",
      "Iteration: 30\n",
      "Weight: -5.190816747823782e+31\n",
      "Bias: -5.190816747823782e+31\n",
      "Error: 2.4309966849597863e+67\n",
      "Iteration: 40\n",
      "Weight: -7.489744898374618e+41\n",
      "Bias: -7.489744898374618e+41\n",
      "Error: 5.0611245364641525e+87\n",
      "Iteration: 50\n",
      "Weight: -1.0806830864573797e+52\n",
      "Bias: -1.0806830864573797e+52\n",
      "Error: 1.0536822913858977e+108\n",
      "Iteration: 60\n",
      "Weight: -1.5593000151560493e+62\n",
      "Bias: -1.5593000151560493e+62\n",
      "Error: 2.1936752656078408e+128\n",
      "Iteration: 70\n",
      "Weight: -2.249888582263424e+72\n",
      "Bias: -2.249888582263424e+72\n",
      "Error: 4.5670418970505597e+148\n",
      "Iteration: 80\n",
      "Weight: -3.2463275722425565e+82\n",
      "Bias: -3.2463275722425565e+82\n",
      "Error: 9.508185653739273e+168\n",
      "Iteration: 90\n",
      "Weight: -4.684073153391537e+92\n",
      "Bias: -4.684073153391537e+92\n",
      "Error: 1.979521897628269e+189\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Stochastic Gradient Descent \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import sympy as sym\n",
    "from sympy.abc import x\n",
    "\n",
    "# define constants\n",
    "INPUT_NOTATION = 7*x + 2 \n",
    "SIZE = 1000\n",
    "\n",
    "# train data\n",
    "train_x = (np.random.rand(SIZE) - 0.5) * 10\n",
    "train_y = np.zeros_like(train_x)\n",
    "\n",
    "def function(input_algebraic_notation, input_value):\n",
    "  polynomial_func = sym.poly(input_algebraic_notation)\n",
    "  plugged_in_val = polynomial_func.subs(x, input_value)\n",
    "  return plugged_in_val\n",
    "\n",
    "for i in range(SIZE):\n",
    "  train_y[i] = function(INPUT_NOTATION, train_x[i])\n",
    "\n",
    "# initialize\n",
    "weight, bias = 0.0, 0.0\n",
    "lr_rate = 1e-2\n",
    "int_number_data = 10\n",
    "errors = []\n",
    "\n",
    "for i in range(100):\n",
    "  # calculate gradient\n",
    "  gradient_weight = np.sum(train_y - weight - bias * train_x)\n",
    "  gradient_bias = np.sum(train_y - weight - bias * train_x)\n",
    "  \n",
    "  # update weight and bias\n",
    "  weight -= lr_rate * gradient_weight\n",
    "  bias -= lr_rate * gradient_bias\n",
    "  \n",
    "  # calculate error\n",
    "  error = np.sum(np.square(train_y - weight - bias * train_x))\n",
    "  errors.append(error)\n",
    "\n",
    "  if i % int_number_data == 0:\n",
    "    print(f\"Iteration: {i}\")\n",
    "    print(f\"Weight: {weight}\")\n",
    "    print(f\"Bias: {bias}\")\n",
    "    print(f\"Error: {error}\")\n",
    "\n",
    "  # if error is less than 0.001, break\n",
    "  if error < 0.001:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0964e3f95126ddd6db105c73e10d4f1245c8e331a9ca081f1675c5dba7db3ce0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
